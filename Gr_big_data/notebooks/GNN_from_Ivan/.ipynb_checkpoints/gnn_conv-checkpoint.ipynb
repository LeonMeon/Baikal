{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = '/home/ivkhar/Baikal/data/mc_baikal_norm_cut-0_ordered_equal_big.h5'\n",
    "\n",
    "max_len = 110\n",
    "file = h5f \n",
    "regime = 'train' \n",
    "batch_size = 32\n",
    "return_reminder = True \n",
    "k_nearest = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator without shuffling\n",
    "# yields (data, labels, adjacency)\n",
    "class generator_no_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data'].shape[0]\n",
    "            self.data_length = hf[self.regime+'/data'].shape[1]     \n",
    "        self.batch_num = self.num // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = self.batch_num*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = 0\n",
    "        stop = self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            for i in range(self.batch_num):\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels, self.full_adj*mask_to_adj )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels, self.full_adj*mask_to_adj )\n",
    "                \n",
    "# generator with shuffling\n",
    "class generator_with_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, buffer_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        self.buffer_size = buffer_size\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data/'].shape[0]\n",
    "            self.data_length = hf[self.regime+'/data'].shape[1]\n",
    "        self.batch_num = (self.num-self.buffer_size) // self.batch_size\n",
    "        self.last_batches_num = self.buffer_size // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = (self.batch_num+self.last_batches_num)*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = self.buffer_size\n",
    "        stop = self.buffer_size + self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            mask = hf[self.regime+'/mask'][:self.buffer_size]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "            buffer_data = np.concatenate((hf[self.regime+'/data'][:self.buffer_size],mask_channel), axis=-1)\n",
    "            buffer_labels = hf[self.regime+'/labels_signal_noise'][:self.buffer_size]\n",
    "            buffer_adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            for i in range(self.batch_num):\n",
    "                idxs = rd.sample( range(self.buffer_size), k=self.batch_size )\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                buffer_data[idxs] = np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                buffer_labels[idxs] = labels\n",
    "                adj = self.full_adj*mask_channel*mask_channel_2\n",
    "                buffer_adj[idxs] = adj\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            # fill the buffer with left data, if any\n",
    "            mask = hf[self.regime+'/mask'][start:stop]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "            buffer_data = np.concatenate( (buffer_data,np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)), axis=0 )\n",
    "            labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "            buffer_labels = np.concatenate( (buffer_labels,labels), axis=0 )\n",
    "            adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            buffer_adj = np.concatenate( (buffer_adj,adj), axis=0 )\n",
    "            sh_idxs = rd.sample( range(buffer_labels.shape[0]), k=buffer_labels.shape[0] )\n",
    "            start = 0\n",
    "            stop = self.batch_size\n",
    "            for i in range(self.last_batches_num):\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "def make_datasets(h5f,make_generator_shuffle,return_batch_reminder,train_batch_size,train_buffer_size,test_batch_size,k_nearest):\n",
    "    # generator for training data\n",
    "    if make_generator_shuffle:\n",
    "        tr_generator = generator_with_shuffle(h5f,'train',train_batch_size,train_buffer_size,return_batch_reminder,k_nearest)\n",
    "    else:\n",
    "        tr_generator = generator_no_shuffle(h5f,'train',train_batch_size,return_batch_reminder,k_nearest)\n",
    "    if return_batch_reminder:\n",
    "        # size of the last batch is unknown\n",
    "        tr_batch_size = None\n",
    "    else:\n",
    "        tr_batch_size = train_batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator( tr_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                         tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "\n",
    "    if make_generator_shuffle:\n",
    "        train_dataset = train_dataset.repeat(-1).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_dataset = train_dataset.repeat(-1).shuffle(num_batch_shuffle)\n",
    "\n",
    "    # generator for validation data\n",
    "    te_generator = generator_no_shuffle(h5f,'test',test_batch_size,False,k_nearest)\n",
    "    te_batch_size = tr_batch_size\n",
    "    test_dataset = tf.data.Dataset.from_generator( te_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                          tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "    \n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0aeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = make_datasets(h5f,True,False,batch_size,1000*batch_size,batch_size,k_nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.lr = tf.keras.layers.Dense(units)\n",
    "        self.act = activation\n",
    "\n",
    "    def call(self, data, adj):\n",
    "        # linea transform features\n",
    "        transf_fts = self.lr(data)\n",
    "        # aggregate information via adj matrix\n",
    "        ret_fts = tf.matmul(adj, transf_fts)\n",
    "        #print(ret_fts.shape)\n",
    "        return self.act(ret_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b74412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no attention\n",
    "class GraphConvModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hid_units_s, hid_act, out_units, out_act):\n",
    "        super(GraphConvModel, self).__init__()\n",
    "        self.hid_layers = []\n",
    "        for un in hid_units_s:\n",
    "            self.hid_layers.append(GraphConvLayer(un, hid_act))\n",
    "        self.out_layer = GraphConvLayer(out_units, out_act)\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn, metrics):\n",
    "        super(GraphConvModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.metrics_ = metrics\n",
    "        self.all_metrics = metrics+[self.loss_tracker]\n",
    "        \n",
    "    def norm_adj(self, adj):\n",
    "        degree = tf.reduce_sum(adj, axis=-1)\n",
    "        norm_degree = tf.linalg.diag(1./tf.sqrt(degree))\n",
    "        n_adj = tf.matmul(norm_degree, tf.matmul(adj, norm_degree) )\n",
    "        return n_adj\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        adj = self.norm_adj(adj)\n",
    "        mask = x[:,:,-1:]\n",
    "        for gr_lr in self.hid_layers:\n",
    "            x = gr_lr(x, adj)\n",
    "        x = self.out_layer(x, adj)\n",
    "        preds = tf.where( tf.cast(mask,bool), tf.keras.layers.Softmax(axis=-1)(x), tf.constant([0.,1.]) )\n",
    "        return preds\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.all_metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.call(datas)\n",
    "            loss = self.loss_fn(labels,preds)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients( zip(grads, self.trainable_weights) )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        preds = self.call(datas)\n",
    "        loss = self.loss_fn(labels,preds)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_units_s = [8]\n",
    "hid_act = tf.keras.activations.selu\n",
    "out_units = 2 \n",
    "out_act = tf.keras.activations.softmax\n",
    "\n",
    "gnn = GraphConvModel(hid_units_s, hid_act, out_units, out_act)\n",
    "\n",
    "lr = 0.001\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "gnn.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## why test loss fixed???\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=100, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243daca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b38e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a5946e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2dd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f0348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956d84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4477f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754f9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed02fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
