{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9c0a1e",
   "metadata": {},
   "source": [
    "Возможные изменения:\n",
    "\n",
    "Мастер-вершина\n",
    "\n",
    "Reduce adj matrix to real vert only\n",
    "\n",
    "e_ij - кодировка ребр; дорого по памяти\n",
    "\n",
    "U-GNN - свертки графа с уменьшением размерности: use clustering?\n",
    "\n",
    "RNN in GNN?\n",
    "\n",
    "U-net cnn: 99.66%\n",
    "\n",
    "LR: ???\n",
    "\n",
    "pre-rnn stabilizes training\n",
    "\n",
    "graph-nn in unet (conv+gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccb9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef64bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22741b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:47:24.773878: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 14:47:25.360308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14738 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:44:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "h5f = '/home/ivkhar/Baikal/data/mc_baikal_norm_cut-0_ordered_equal_big.h5'\n",
    "max_len = 110\n",
    "batch_size = 32\n",
    "\n",
    "# generator without shuffling\n",
    "class generator_no_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, return_reminder):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data'].shape[0]     \n",
    "        self.batch_num = self.num // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = self.batch_num*self.batch_size\n",
    "\n",
    "    def __call__(self):\n",
    "        start = 0\n",
    "        stop = self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            for i in range(self.batch_num):\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels )\n",
    "\n",
    "# generator with shuffling\n",
    "class generator_with_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, buffer_size, return_reminder):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        self.buffer_size = buffer_size\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data/'].shape[0] \n",
    "        self.batch_num = (self.num-self.buffer_size) // self.batch_size\n",
    "        self.last_batches_num = self.buffer_size // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = (self.batch_num+self.last_batches_num)*self.batch_size\n",
    "\n",
    "    def __call__(self):\n",
    "        start = self.buffer_size\n",
    "        stop = self.buffer_size + self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            mask = hf[self.regime+'/mask'][:self.buffer_size]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            buffer_data = np.concatenate((hf[self.regime+'/data'][:self.buffer_size],mask_channel), axis=-1)\n",
    "            buffer_labels = hf[self.regime+'/labels_signal_noise'][:self.buffer_size]\n",
    "            for i in range(self.batch_num):\n",
    "                idxs = rd.sample( range(self.buffer_size), k=self.batch_size )\n",
    "                yield ( buffer_data[idxs] , buffer_labels[idxs] )\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                buffer_data[idxs] = np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                buffer_labels[idxs] = labels\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            # fill the buffer with left data, if any\n",
    "            # this a bit increases buffer size and MIGT BE COSTLY\n",
    "            mask = hf[self.regime+'/mask'][start:stop]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            buffer_data = np.concatenate( (buffer_data,np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)), axis=0 )\n",
    "            labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "            buffer_labels  = np.concatenate( (buffer_labels,labels), axis=0 )\n",
    "            sh_idxs = rd.sample( range(buffer_labels.shape[0]), k=buffer_labels.shape[0] )\n",
    "            start = 0\n",
    "            stop = self.batch_size\n",
    "            for i in range(self.last_batches_num):\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs] )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs])\n",
    "\n",
    "### MADE SO BATCH SIZE IS CONSTANT AND NO REMINDER\n",
    "def make_datasets(h5f,make_generator_shuffle,return_batch_reminder,train_batch_size,train_buffer_size,test_batch_size):\n",
    "    # generator for training data\n",
    "    if make_generator_shuffle:\n",
    "        tr_generator = generator_with_shuffle(h5f,'train',train_batch_size,train_buffer_size,return_batch_reminder)\n",
    "    else:\n",
    "        tr_generator = generator_no_shuffle(h5f,'train',train_batch_size,return_batch_reminder)\n",
    "    if return_batch_reminder:\n",
    "        # size of the last batch is unknown\n",
    "        tr_batch_size = None\n",
    "    else:\n",
    "        tr_batch_size = train_batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator( tr_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2))\n",
    "                                         ) )\n",
    "\n",
    "    if make_generator_shuffle:\n",
    "        train_dataset = train_dataset.repeat(-1).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_dataset = train_dataset.repeat(-1).shuffle(num_batch_shuffle)\n",
    "\n",
    "    # generator for validation data\n",
    "    te_generator = generator_no_shuffle(h5f,'test',test_batch_size,False)\n",
    "    te_batch_size = tr_batch_size\n",
    "    test_dataset = tf.data.Dataset.from_generator( te_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2))\n",
    "                                          ) )\n",
    "    \n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = make_datasets(h5f,True,False,batch_size,500*batch_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca388491",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pre-transforms nodes encodings\n",
    "# returns transformed encoding plus initial\n",
    "# pass empty filters to avoid new encs\n",
    "# in case k!=1 neighbours effect the final encoding\n",
    "\n",
    "# (bs,om,c) -> (bs,om,c')\n",
    "class NodesEncoder(tf.keras.layers.Layer):\n",
    "      \n",
    "    def __init__(self, filters, kernels, activation):\n",
    "        super(NodesEncoder, self).__init__()\n",
    "        assert len(filters)==len(kernels)\n",
    "        self.num_layers = len(filters)\n",
    "        self.conv_layers = [ tf.keras.layers.Conv1D(f, k, padding='same') for (f,k) in zip(filters,kernels) ]\n",
    "        self.activation = activation\n",
    "        self.filters = filters\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        if self.num_layers==0:\n",
    "            self.out_encs_length = input_shape[-1]\n",
    "        else:\n",
    "            self.out_encs_length = self.filters[-1]+input_shape[-1]\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        init_x = x\n",
    "        for conv in self.conv_layers:\n",
    "            x = conv(x)\n",
    "            x = self.activation(x)\n",
    "            x = tf.concat((x,init_x),axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9885b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creates messages for pairs of nodes\n",
    "# k=(1,1) reduces to usual dense\n",
    "\n",
    "# (bs,om,om,c) -> (bs,om,om,c')\n",
    "class MessageCreator(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, kernels, activation):\n",
    "        super(MessageCreator, self).__init__()\n",
    "        self.conv_layers = [ tf.keras.layers.Conv2D(f, k, padding='same') for (f,k) in zip(filters,kernels) ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, x):\n",
    "        for lr in self.conv_layers:\n",
    "            x = lr(x)\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33ada1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### message parser\n",
    "# reduces all messages to single message\n",
    "\n",
    "# (bs,om,om,c) -> (bs,om,c')\n",
    "class MessageParser(tf.keras.layers.Layer):\n",
    "  \n",
    "    # filters, kernels, strides - sequances\n",
    "    def __init__(self, filters, kernels, strides, units, activation, take_mean):\n",
    "        super(MessageParser, self).__init__()\n",
    "        assert len(filters)==len(kernels)\n",
    "        assert len(filters)==len(strides)\n",
    "        self.conv_layers = [ tf.keras.layers.Conv2D(f, k, strides=s, padding='same') for (f,k,s) in zip(filters,kernels,strides) ]\n",
    "        self.dence_layers = [ tf.keras.layers.Dense(un) for un in units ]\n",
    "        self.activation = activation\n",
    "        self.max_poolings = [  tf.keras.layers.MaxPooling2D(pool_size=(1,2)) for f in filters ]\n",
    "        self.take_mean = take_mean\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for (conv,mp) in zip(self.conv_layers,self.max_poolings):\n",
    "            x = conv(x)\n",
    "            x = self.activation(x)\n",
    "            x = mp(x)\n",
    "        # (bs,om,om',c')\n",
    "        if self.take_mean:\n",
    "            x = tf.math.reduce_mean(x, axis=2) # (bs,om,c')\n",
    "        else:\n",
    "            x = tf.reshape(x, (x.shape[0],x.shape[1],-1)) # (bs,om,c')\n",
    "        for de in self.dence_layers:\n",
    "            x = de(x)\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eecdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### attention calculator\n",
    "# takes pairs of channels info and calculates relevance\n",
    "# k=(1,1) reduces to usual dense\n",
    "\n",
    "# (b,om,om,c) -> (b,om,om,1)\n",
    "class AttentionEstablisher(tf.keras.layers.Layer):\n",
    "  \n",
    "    def __init__(self, filters, kernels, activations):\n",
    "        super(AttentionEstablisher, self).__init__()\n",
    "        self.conv_layers = [ tf.keras.layers.Conv2D(f, k, padding='same') for (f,k) in zip(filters,kernels) ]\n",
    "        self.activations = [ act for act in activations ]\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for (lr,ac) in zip(self.conv_layers,self.activations):\n",
    "            x = lr(x)\n",
    "            x = ac(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54a597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### state updater\n",
    "\n",
    "# (bs,om,c) - > (bs,om,c')\n",
    "class StateUpdater(tf.keras.layers.Layer):\n",
    "  \n",
    "    def __init__(self, units, activations):\n",
    "        super(StateUpdater, self).__init__()\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(u) for u in units ]\n",
    "        self.activations = [ act for act in activations ]\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for (dense,ac) in zip(self.dense_layers,self.activations):\n",
    "            x = dense(x)\n",
    "            x = ac(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a2cf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphStepLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes_encoder, attention_layer, message_creator, message_parser, state_updater, data_length, batch_size):\n",
    "        super(GraphStepLayer, self).__init__()\n",
    "        self.nodes_encoder = nodes_encoder\n",
    "        self.attention_layer = attention_layer\n",
    "        self.message_creator = message_creator\n",
    "        self.message_parser = message_parser\n",
    "        self.state_updater = state_updater\n",
    "        self.pos_channel = np.expand_dims(np.expand_dims(np.arange(-1,1,2/data_length),axis=-1),axis=0)\n",
    "        self.data_length = data_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nodes_encoder.build(input_shape)\n",
    "        \n",
    "    # (bs,om,c) -> (bs,om,om,2c)\n",
    "    def make_pairs(self, vert):\n",
    "        vert_exp_1 = tf.expand_dims(vert,axis=1) # (bs,1,om,c)\n",
    "        vert_exp_2 = tf.expand_dims(vert,axis=2) # (bs,om,1,c)\n",
    "        adj = tf.fill( (vert.shape[0],vert.shape[1],vert.shape[1],1), 1. )\n",
    "        rel_vert = adj*vert_exp_2 # (bs, om, om_targ, c)\n",
    "        base_vert = adj*vert_exp_1 # (bs, om_base, om, c)\n",
    "        pairs = tf.concat((rel_vert,base_vert),axis=-1) # (bs, om_base, om_target, 2c)\n",
    "        return pairs\n",
    "        \n",
    "    def call(self, inputs):\n",
    "\n",
    "        ### encode nodes for message passing\n",
    "        node_encs = self.nodes_encoder(inputs) # (bs,om,c)\n",
    "\n",
    "        ### calculate attentions\n",
    "        # add position channels\n",
    "        pos_encs = np.repeat( self.pos_channel, inputs.shape[0], axis=0 )\n",
    "        node_encs_att = tf.concat( (node_encs,pos_encs), axis=-1 ) # (bs,om,c)\n",
    "        \n",
    "        ### calculate attentions \n",
    "        pairs = self.make_pairs( node_encs_att ) # (b,om,om',2c)\n",
    "        attentions = self.attention_layer( pairs ) # (b,om,om,1)\n",
    "\n",
    "        ### prepare message\n",
    "        messages = self.message_creator(attentions*pairs) # (b,om,om,c')\n",
    "        mess = self.message_parser(messages) # (b,om,c)\n",
    "\n",
    "        ### update states\n",
    "        update_from = tf.concat((node_encs,mess), axis=-1) # (b, om, ch')\n",
    "        out_encs = self.state_updater( update_from ) # (b, om, ch)\n",
    "        \n",
    "        return out_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b98062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphResLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes_encoders, attention_layers, message_creators, message_parsers, state_updaters, data_length, batch_size):\n",
    "        super(GraphResLayer, self).__init__()\n",
    "        self.gr_layers = []\n",
    "        for i in range(2):\n",
    "            gr_layer = GraphStepLayer( nodes_encoders[i], attention_layers[i], message_creators[i], message_parsers[i], state_updaters[i], data_length, batch_size)\n",
    "            self.gr_layers.append(gr_layer)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # pass through fist layer\n",
    "        r1 = self.gr_layers[0](inputs)\n",
    "        # second\n",
    "        r2 = self.gr_layers[1](r1)\n",
    "        # concat\n",
    "        res = tf.concat((r1,r2), axis=-1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0faaee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLikeModelRes(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, pre_layer, gnn_layers, post_layer):\n",
    "        super(GraphLikeModelRes, self).__init__()\n",
    "        self.pre_layer = pre_layer\n",
    "        self.graph_layers = gnn_layers\n",
    "        self.post_layer = post_layer\n",
    "            \n",
    "    def call(self, x):\n",
    "        mask = x[:,:,-1:]\n",
    "        x = self.pre_layer(x)\n",
    "        for gr_lr in self.graph_layers:\n",
    "            x = gr_lr(x)\n",
    "        x = self.post_layer(x)\n",
    "        preds = tf.where( tf.cast(mask,bool), x, tf.constant([0.,1.]) )\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788d4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, lr):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ae434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "selu = tf.keras.activations.selu\n",
    "relu = tf.keras.activations.relu\n",
    "sigmoid = tf.keras.activations.sigmoid\n",
    "softmax = tf.keras.activations.softmax\n",
    "\n",
    "depth = 1\n",
    "\n",
    "# create nodes encoders\n",
    "node_enc_filters = [[64] for i in range(depth)]\n",
    "node_enc_kernels =  [[1] for i in range(depth)]\n",
    "node_enc_activation = selu\n",
    "nodes_encoders = []\n",
    "for (filters,kernels) in zip(node_enc_filters,node_enc_kernels):\n",
    "    nodes_encoders.append( [NodesEncoder(filters,kernels,node_enc_activation),NodesEncoder(filters,kernels,node_enc_activation)] )\n",
    "\n",
    "# make message creators\n",
    "# 2d kernels\n",
    "mess_cr_filters = [[64] for i in range(depth)]\n",
    "mess_cr_kernels = [[(1,1)] for i in range(depth)]\n",
    "mess_cr_activation = selu\n",
    "mess_crs = []\n",
    "for (filters,kernels) in zip(mess_cr_filters,mess_cr_kernels):\n",
    "    mess_crs.append( [MessageCreator(filters,kernels,mess_cr_activation),MessageCreator(filters,kernels,mess_cr_activation)] )\n",
    "\n",
    "# create message parsers\n",
    "# 2d kernels and strides\n",
    "mess_pars_filters = [16]\n",
    "mess_pars_kernels = [(1,5)]\n",
    "mess_pars_strides = [(1,2)]\n",
    "mess_pars_units = [[64] for i in range(depth)]\n",
    "mess_pass_activation = selu\n",
    "mess_pars_take_mean = False\n",
    "mess_parsers = []\n",
    "for units in mess_pars_units:\n",
    "    mess_parsers.append( [MessageParser(mess_pars_filters,mess_pars_kernels,mess_pars_strides,units,mess_pass_activation,mess_pars_take_mean),\n",
    "                         MessageParser(mess_pars_filters,mess_pars_kernels,mess_pars_strides,units,mess_pass_activation,mess_pars_take_mean)] )\n",
    "\n",
    "# make attention calculators\n",
    "# 2d kernels\n",
    "att_filters = [[32,1] for i in range(depth)]\n",
    "att_kernels = [[(1,1),(1,1)] for i in range(depth)]\n",
    "att_activations = [[selu,sigmoid] for i in range(depth)]\n",
    "atts = []\n",
    "for (filters,kernels,act) in zip(att_filters,att_kernels,att_activations):\n",
    "    atts.append( [AttentionEstablisher(filters,kernels,act),AttentionEstablisher(filters,kernels,act)] )\n",
    "\n",
    "# make state updaters\n",
    "state_upd_units = [[64] for i in range(depth)]\n",
    "state_upd_activations = [[selu] for i in range(depth)]\n",
    "state_ups = []\n",
    "for (units,act) in zip(state_upd_units,state_upd_activations):\n",
    "    state_ups.append( [StateUpdater(units,act),StateUpdater(units,act)] )\n",
    "\n",
    "# make graph layers\n",
    "gr_layers = []\n",
    "for (node_enc,mess_cr,mess_pars,att_est,state_upd) in zip(nodes_encoders[:-1],mess_crs[:-1],mess_parsers[:-1],atts[:-1],state_ups[:-1]):\n",
    "    gr_layers.append( GraphResLayer(node_enc, att_est, mess_cr, mess_pars, state_upd, max_len, batch_size) )\n",
    "    \n",
    "# make pre-layers\n",
    "pre_layer = tf.identity\n",
    "#pre_layer = tf.keras.layers.LSTM(32, activation='tanh', recurrent_activation='sigmoid', return_sequences=True)\n",
    "\n",
    "# make post_layers\n",
    "post_layer = tf.keras.layers.Dense(2, activation=softmax)\n",
    "\n",
    "# make gnn\n",
    "gnn = GraphLikeModelRes( pre_layer, gr_layers, post_layer)\n",
    "gnn = compile_model(gnn, 0.01)\n",
    "\n",
    "print(len(gr_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51abb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 20:45:01.562255: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2022-05-03 20:45:02.828614: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-05-03 20:45:03.008817: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 253s 98ms/step - loss: 0.0471 - accuracy: 0.9803 - val_loss: 0.0362 - val_accuracy: 0.9841\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 245s 98ms/step - loss: 0.0252 - accuracy: 0.9895 - val_loss: 0.0236 - val_accuracy: 0.9903\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 244s 98ms/step - loss: 0.0204 - accuracy: 0.9917 - val_loss: 0.0246 - val_accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 244s 98ms/step - loss: 0.0184 - accuracy: 0.9925 - val_loss: 0.0221 - val_accuracy: 0.9910\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 245s 98ms/step - loss: 0.0172 - accuracy: 0.9930 - val_loss: 0.0209 - val_accuracy: 0.9915\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 244s 98ms/step - loss: 0.0163 - accuracy: 0.9934 - val_loss: 0.0196 - val_accuracy: 0.9922\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 243s 97ms/step - loss: 0.0159 - accuracy: 0.9936 - val_loss: 0.0189 - val_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 243s 97ms/step - loss: 0.0155 - accuracy: 0.9937 - val_loss: 0.0197 - val_accuracy: 0.9921\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 243s 97ms/step - loss: 0.0149 - accuracy: 0.9940 - val_loss: 0.0184 - val_accuracy: 0.9926\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 242s 97ms/step - loss: 0.0146 - accuracy: 0.9941 - val_loss: 0.0187 - val_accuracy: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f189078cb50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with rnn\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d1ceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 230s 91ms/step - loss: 0.0379 - accuracy: 0.9843 - val_loss: 0.0240 - val_accuracy: 0.9909\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0183 - accuracy: 0.9930 - val_loss: 0.0189 - val_accuracy: 0.9928\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0148 - accuracy: 0.9942 - val_loss: 0.0179 - val_accuracy: 0.9932\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0134 - accuracy: 0.9948 - val_loss: 0.0156 - val_accuracy: 0.9939\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 228s 91ms/step - loss: 0.0126 - accuracy: 0.9951 - val_loss: 0.0159 - val_accuracy: 0.9938\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0149 - val_accuracy: 0.9943\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0115 - accuracy: 0.9955 - val_loss: 0.0146 - val_accuracy: 0.9943\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0109 - accuracy: 0.9958 - val_loss: 0.0132 - val_accuracy: 0.9950\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 226s 90ms/step - loss: 0.0108 - accuracy: 0.9958 - val_loss: 0.0139 - val_accuracy: 0.9946\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 225s 90ms/step - loss: 0.0103 - accuracy: 0.9960 - val_loss: 0.0127 - val_accuracy: 0.9952\n",
      "Epoch 11/25\n",
      " 280/2500 [==>...........................] - ETA: 3:09 - loss: 0.0102 - accuracy: 0.9960"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1042392/1420059253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# without rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \"\"\"\n\u001b[1;32m   1148\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf27/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# without rnn\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc6eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 196s 77ms/step - loss: 0.0368 - accuracy: 0.9848 - val_loss: 0.0240 - val_accuracy: 0.9909\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 193s 77ms/step - loss: 0.0170 - accuracy: 0.9935 - val_loss: 0.0196 - val_accuracy: 0.9923\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 192s 77ms/step - loss: 0.0148 - accuracy: 0.9943 - val_loss: 0.0173 - val_accuracy: 0.9931\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 194s 78ms/step - loss: 0.0132 - accuracy: 0.9948 - val_loss: 0.0151 - val_accuracy: 0.9941\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 194s 78ms/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.0146 - val_accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 194s 78ms/step - loss: 0.0116 - accuracy: 0.9955 - val_loss: 0.0152 - val_accuracy: 0.9940\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 193s 77ms/step - loss: 0.0111 - accuracy: 0.9957 - val_loss: 0.0139 - val_accuracy: 0.9946\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 201s 80ms/step - loss: 0.0107 - accuracy: 0.9959 - val_loss: 0.0131 - val_accuracy: 0.9949\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 206s 82ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.0135 - val_accuracy: 0.9949\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 199s 80ms/step - loss: 0.0102 - accuracy: 0.9961 - val_loss: 0.0131 - val_accuracy: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17c85c1c10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds from lass gnn layer\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf6dc4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 231s 91ms/step - loss: 0.0391 - accuracy: 0.9837 - val_loss: 0.0292 - val_accuracy: 0.9882\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0224 - accuracy: 0.9910 - val_loss: 0.0251 - val_accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 226s 91ms/step - loss: 0.0179 - accuracy: 0.9929 - val_loss: 0.0184 - val_accuracy: 0.9927\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.0190 - val_accuracy: 0.9927\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 226s 90ms/step - loss: 0.0144 - accuracy: 0.9944 - val_loss: 0.0188 - val_accuracy: 0.9926\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0133 - accuracy: 0.9947 - val_loss: 0.0155 - val_accuracy: 0.9940\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0126 - accuracy: 0.9950 - val_loss: 0.0153 - val_accuracy: 0.9941\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.0149 - val_accuracy: 0.9943\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 228s 91ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0148 - val_accuracy: 0.9942\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 225s 90ms/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 0.0150 - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17c80df970>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds from lass conv layer\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52564fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c31a3a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:47:35.473910: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2022-05-04 14:47:36.788671: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-05-04 14:47:36.982966: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 128s 49ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0240 - val_accuracy: 0.9908\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 125s 50ms/step - loss: 0.0181 - accuracy: 0.9931 - val_loss: 0.0209 - val_accuracy: 0.9920\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 130s 52ms/step - loss: 0.0149 - accuracy: 0.9943 - val_loss: 0.0174 - val_accuracy: 0.9934\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 124s 49ms/step - loss: 0.0135 - accuracy: 0.9948 - val_loss: 0.0177 - val_accuracy: 0.9931\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 140s 56ms/step - loss: 0.0129 - accuracy: 0.9950 - val_loss: 0.0159 - val_accuracy: 0.9939\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 160s 64ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0152 - val_accuracy: 0.9942\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 176s 70ms/step - loss: 0.0198 - accuracy: 0.9930 - val_loss: 0.0213 - val_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 153s 61ms/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.0177 - val_accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 138s 55ms/step - loss: 0.0142 - accuracy: 0.9947 - val_loss: 0.0169 - val_accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 126s 51ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.0167 - val_accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d82e39130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=10, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63188f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4702d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 121s 48ms/step - loss: 0.0288 - accuracy: 0.9888 - val_loss: 0.0216 - val_accuracy: 0.9916\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 124s 50ms/step - loss: 0.0187 - accuracy: 0.9929 - val_loss: 0.0213 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b7428af40>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, relu, sigmoid, take_mean\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a65f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 125s 49ms/step - loss: 0.0288 - accuracy: 0.9887 - val_loss: 0.0238 - val_accuracy: 0.9909\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 124s 50ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.0208 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9bd05f3790>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, relu, sigmoid, no_take_mean\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0663807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 119s 47ms/step - loss: 0.0375 - accuracy: 0.9848 - val_loss: 0.0242 - val_accuracy: 0.9909\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 123s 49ms/step - loss: 0.0188 - accuracy: 0.9929 - val_loss: 0.0202 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b3875b040>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b94e61af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 132s 52ms/step - loss: 0.0364 - accuracy: 0.9854 - val_loss: 0.0252 - val_accuracy: 0.9898\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 126s 51ms/step - loss: 0.0192 - accuracy: 0.9925 - val_loss: 0.0222 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b74254340>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, softmax, no_take_mean\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c49ce201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 120s 47ms/step - loss: 0.0414 - accuracy: 0.9836 - val_loss: 0.0286 - val_accuracy: 0.9887\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 125s 50ms/step - loss: 0.0234 - accuracy: 0.9909 - val_loss: 0.0227 - val_accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b387b8850>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, softmax, no_take_mean, ident@nodes_encoder\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c86d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 118s 46ms/step - loss: 0.0379 - accuracy: 0.9843 - val_loss: 0.0267 - val_accuracy: 0.9894\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 125s 50ms/step - loss: 0.0195 - accuracy: 0.9925 - val_loss: 0.0207 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b38247c40>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean, ident@nodes_encoder\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7013ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 18:36:05.945987: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2022-05-03 18:36:07.069253: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-05-03 18:36:07.247695: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 123s 47ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0258 - val_accuracy: 0.9898\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 130s 52ms/step - loss: 0.0202 - accuracy: 0.9921 - val_loss: 0.0201 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2145b43ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean, mess_cr_k=(3,3)\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6e9028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 402s 160ms/step - loss: 0.0369 - accuracy: 0.9854 - val_loss: 0.0244 - val_accuracy: 0.9904\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 397s 159ms/step - loss: 0.0204 - accuracy: 0.9919 - val_loss: 0.0215 - val_accuracy: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20cc6251c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean, mess_pars_k=(3,5)\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "183a0264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 147s 58ms/step - loss: 0.0343 - accuracy: 0.9864 - val_loss: 0.0298 - val_accuracy: 0.9884\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 140s 56ms/step - loss: 0.0223 - accuracy: 0.9914 - val_loss: 0.0224 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21301f3be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depth 1, selu, sigmoid, no_take_mean, att_k=(3,3)x2\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb915cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f097b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ce6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1794e077",
   "metadata": {},
   "source": [
    "## !!??\n",
    "\n",
    "One layer with high lr was very good. \n",
    "\n",
    "More layers are much worse at the lr.\n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1b9d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"graph_like_model_res_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " graph_res_layer_14 (GraphRe  multiple                 38250     \n",
      " sLayer)                                                         \n",
      "                                                                 \n",
      " graph_res_layer_15 (GraphRe  multiple                 45790     \n",
      " sLayer)                                                         \n",
      "                                                                 \n",
      " graph_res_layer_16 (GraphRe  multiple                 45790     \n",
      " sLayer)                                                         \n",
      "                                                                 \n",
      " dense_72 (Dense)            multiple                  130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129,960\n",
      "Trainable params: 129,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82c0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
