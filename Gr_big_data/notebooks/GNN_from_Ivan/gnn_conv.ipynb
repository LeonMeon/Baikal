{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = '/home/ivkhar/Baikal/data/mc_baikal_norm_cut-0_ordered_equal_big.h5'\n",
    "\n",
    "max_len = 110\n",
    "file = h5f \n",
    "regime = 'train' \n",
    "batch_size = 32\n",
    "return_reminder = True \n",
    "k_nearest = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator without shuffling\n",
    "# yields (data, labels, adjacency)\n",
    "class generator_no_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data'].shape[0]\n",
    "            self.data_length = hf[self.regime+'/data'].shape[1]     \n",
    "        self.batch_num = self.num // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = self.batch_num*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = 0\n",
    "        stop = self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            for i in range(self.batch_num):\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels, self.full_adj*mask_to_adj )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                  labels, self.full_adj*mask_to_adj )\n",
    "                \n",
    "# generator with shuffling\n",
    "class generator_with_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, buffer_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        self.buffer_size = buffer_size\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data/'].shape[0]\n",
    "            self.data_length = hf[self.regime+'/data'].shape[1]\n",
    "        self.batch_num = (self.num-self.buffer_size) // self.batch_size\n",
    "        self.last_batches_num = self.buffer_size // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = (self.batch_num+self.last_batches_num)*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = self.buffer_size\n",
    "        stop = self.buffer_size + self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            mask = hf[self.regime+'/mask'][:self.buffer_size] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 ) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 ) ##!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            buffer_data = np.concatenate((hf[self.regime+'/data'][:self.buffer_size],mask_channel), axis=-1) #\n",
    "            \n",
    "            buffer_labels = hf[self.regime+'/labels_signal_noise'][:self.buffer_size] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            buffer_adj = self.full_adj*mask_channel*mask_channel_2 ####\n",
    "            for i in range(self.batch_num): #\n",
    "                idxs = rd.sample( range(self.buffer_size), k=self.batch_size ) #\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )  #\n",
    "                mask = hf[self.regime+'/mask'][start:stop] #\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 ) #\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 ) #\n",
    "                buffer_data[idxs] = np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)#\n",
    "                labels = hf[self.regime+'/labels_signal_noise'][start:stop] #\n",
    "                buffer_labels[idxs] = labels #\n",
    "                adj = self.full_adj*mask_channel*mask_channel_2#\n",
    "                buffer_adj[idxs] = adj\n",
    "                start += self.batch_size##\n",
    "                stop += self.batch_size\n",
    "            # fill the buffer with left data, if any\n",
    "            mask = hf[self.regime+'/mask'][start:stop]###\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 ) #\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 ) #\n",
    "            buffer_data = np.concatenate( (buffer_data,np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)), axis=0 )\n",
    "            labels = hf[self.regime+'/labels_signal_noise'][start:stop] ##########################\n",
    "            buffer_labels = np.concatenate( (buffer_labels,labels), axis=0 )\n",
    "            adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            buffer_adj = np.concatenate( (buffer_adj,adj), axis=0 )\n",
    "            sh_idxs = rd.sample( range(buffer_labels.shape[0]), k=buffer_labels.shape[0] )\n",
    "            start = 0\n",
    "            stop = self.batch_size\n",
    "            for i in range(self.last_batches_num):\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "def make_datasets(h5f,make_generator_shuffle,return_batch_reminder,train_batch_size,train_buffer_size,test_batch_size,k_nearest):\n",
    "    # generator for training data\n",
    "    if make_generator_shuffle:\n",
    "        tr_generator = generator_with_shuffle(h5f,'train',train_batch_size,train_buffer_size,return_batch_reminder,k_nearest)\n",
    "    else:\n",
    "        tr_generator = generator_no_shuffle(h5f,'train',train_batch_size,return_batch_reminder,k_nearest)\n",
    "    if return_batch_reminder:\n",
    "        # size of the last batch is unknown\n",
    "        tr_batch_size = None\n",
    "    else:\n",
    "        tr_batch_size = train_batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator( tr_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                         tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "\n",
    "    if make_generator_shuffle:\n",
    "        train_dataset = train_dataset.repeat(-1).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_dataset = train_dataset.repeat(-1).shuffle(num_batch_shuffle)\n",
    "\n",
    "    # generator for validation data\n",
    "    te_generator = generator_no_shuffle(h5f,'test',test_batch_size,False,k_nearest)\n",
    "    te_batch_size = tr_batch_size\n",
    "    test_dataset = tf.data.Dataset.from_generator( te_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                          tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "    \n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0aeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = make_datasets(h5f,True,False,batch_size,1000*batch_size,batch_size,k_nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.lr = tf.keras.layers.Dense(units)\n",
    "        self.act = activation\n",
    "\n",
    "    def call(self, data, adj):\n",
    "        # linea transform features\n",
    "        transf_fts = self.lr(data)\n",
    "        # aggregate information via adj matrix\n",
    "        ret_fts = tf.matmul(adj, transf_fts)\n",
    "        #print(ret_fts.shape)\n",
    "        return self.act(ret_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b74412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no attention\n",
    "class GraphConvModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hid_units_s, hid_act, out_units, out_act):\n",
    "        super(GraphConvModel, self).__init__()\n",
    "        self.hid_layers = []\n",
    "        for un in hid_units_s:\n",
    "            self.hid_layers.append(GraphConvLayer(un, hid_act))\n",
    "        self.out_layer = GraphConvLayer(out_units, out_act)\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn, metrics):\n",
    "        super(GraphConvModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.metrics_ = metrics\n",
    "        self.all_metrics = metrics+[self.loss_tracker]\n",
    "        \n",
    "    def norm_adj(self, adj):\n",
    "        degree = tf.reduce_sum(adj, axis=-1)\n",
    "        norm_degree = tf.linalg.diag(1./tf.sqrt(degree))\n",
    "        n_adj = tf.matmul(norm_degree, tf.matmul(adj, norm_degree) )\n",
    "        return n_adj\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        adj = self.norm_adj(adj)\n",
    "        mask = x[:,:,-1:]\n",
    "        for gr_lr in self.hid_layers:\n",
    "            x = gr_lr(x, adj)\n",
    "        x = self.out_layer(x, adj)\n",
    "        preds = tf.where( tf.cast(mask,bool), tf.keras.layers.Softmax(axis=-1)(x), tf.constant([0.,1.]) )\n",
    "        return preds\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.all_metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.call(datas)\n",
    "            loss = self.loss_fn(labels,preds)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients( zip(grads, self.trainable_weights) )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        preds = self.call(datas)\n",
    "        loss = self.loss_fn(labels,preds)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_units_s = [8]\n",
    "hid_act = tf.keras.activations.selu\n",
    "out_units = 2 \n",
    "out_act = tf.keras.activations.softmax\n",
    "\n",
    "gnn = GraphConvModel(hid_units_s, hid_act, out_units, out_act)\n",
    "\n",
    "lr = 0.001\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "gnn.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## why test loss fixed???\n",
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=100, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5243daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75+180-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b38e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7401599999999999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**5-x**4+x**3-x**2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a408b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=-0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e2dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.-20.-30.-40\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "s = input().split(\".\")\n",
    "if len(s) != 4:\n",
    "    answer = \"false\"\n",
    "else:\n",
    "    answer = \"true\"\n",
    "    for i in s:\n",
    "        if float(i)%1 != 0 or int(i)<0 or int(i)>255:\n",
    "            answer = \"false\"\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'I':'1','V':'5','X':'10','L':'50','C':'100','M':'1000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8956d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "word = input()\n",
    "d = {\"I\":1,\"V\":5,\"X\":10,\"L\":50,\"C\":100,\"D\":500,\"M\":1000}\n",
    "arab = 0\n",
    "for i in range(len(word)):\n",
    "    try:\n",
    "        if d[word[i]] < d[word[i+1]]:\n",
    "            arab -= d[word[i]]\n",
    "        else:\n",
    "            arab += d[word[i]]\n",
    "    except KeyError:\n",
    "        arab = 0\n",
    "        break\n",
    "    except IndexError:\n",
    "        arab += d[word[i]]\n",
    "print(arab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4477f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):return list(a[0])+f(zip(*a[1:])[::-1])if a else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3754f9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "63 83 3\n",
      "23 43 22\n",
      "33 44 55\n",
      "63 23 33 44 55 22 3 83 43 "
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "array =[]\n",
    "for i in range(n):\n",
    "    array.append(list(map(int,input().split(\" \"))))\n",
    "#print(array)\n",
    "\n",
    "direction=[-1,0]\n",
    "for i in range(0,n-1):\n",
    "    a= [array[j][i] for j in range(i,n-1)]\n",
    "    print(a*,end = \" \")\n",
    "    a = [array[-(i+1)][j] for j in range(i,n-1-i)]\n",
    "    print(a*,end = \" \")\n",
    "    a = [array[j][-(i+1)] for j in range(n-1-i,i,-1)]\n",
    "    print(a*,end = \" \")\n",
    "    a = [array[i][j] for j in range(n-i-1,i,-1)]\n",
    "    print(a*,end = \" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3e4cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 23] [33, 44] [55, 22] [3, 83] "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print( [array[j][i] for j in range(i,n-1)], end = \" \")\n",
    "print([array[-(i+1)][j] for j in range(i,n-1-i)], end = \" \")\n",
    "print( [array[j][-(i+1)] for j in range(n-1-i,i,-1)], end = \" \")\n",
    "print(  [array[i][j] for j in range(n-i-1,i,-1)], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfbd9132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[63, 83, 3], [23, 43, 22], [33, 44, 55]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d6e8e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63, 83]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[array[0][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bed02fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3 / 3/7\n",
      "2/3 / 3/7 = 14/9\n"
     ]
    }
   ],
   "source": [
    "from math import gcd\n",
    "s = input()\n",
    "first,sign,second = s.split(\" \")\n",
    "first_up ,first_down = list(map(int,first.split(\"/\")))\n",
    "second_up ,second_down = list(map(int,second.split(\"/\")))\n",
    "#print(first_up ,first_down,second_up ,second_down,sign)\n",
    "if sign == \"+\":\n",
    "    numerator =  first_up * second_down + second_up * first_down\n",
    "    denominator = first_down * second_down\n",
    "elif sign == \"-\":\n",
    "    numerator =  first_up * second_down - second_up * first_down\n",
    "    denominator = first_down * second_down    \n",
    "elif sign == \"*\":\n",
    "    numerator =  first_up * second_up\n",
    "    denominator = first_down * second_down\n",
    "elif sign == \"/\":\n",
    "    numerator =  first_up * second_down\n",
    "    denominator = first_down * second_up \n",
    "nod = gcd(numerator, denominator)\n",
    "numerator //= nod\n",
    "denominator //= nod\n",
    "\n",
    "print(s+\" =\",str(numerator)+\"/\"+str(denominator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "663f1857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "())(()\n",
      "false\n"
     ]
    }
   ],
   "source": [
    "s = input()\n",
    "n = 0\n",
    "answer = 'true'\n",
    "for i in s:\n",
    "    if i == \"(\":\n",
    "        n += 1\n",
    "    else :\n",
    "        n -= 1\n",
    "    if n < 0:\n",
    "        answer = \"false\"\n",
    "        break\n",
    "if n != 0:\n",
    "    print('false')\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef803ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
