{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b0ac8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12092/1969946970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f68604",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7723918",
   "metadata": {},
   "source": [
    "### Routines for data generators\n",
    "\n",
    "To construct adjacency matrix, we consider hits as an ordered set, according to the time of the activation. Then, we say that two verticies are connected if they are separeted by not more than k cells (parameter k_nearest).\n",
    "\n",
    "Note that adj matrix is prepared in generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff55faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 1694740\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "h5f = \"/home/leonov/Baikal/Cut_8_nu/APRIL/Ordered/Data/mc_baikal_norm_cut-8_ordered_equal_big.h5\"\n",
    "\n",
    "max_len = 32 #110\n",
    "file = h5f \n",
    "regime = 'train' \n",
    "batch_size = 32\n",
    "return_reminder = True \n",
    "k_nearest = 2\n",
    "with h5.File(h5f,'r') as hf:\n",
    "    print(hf[regime+'/data'].shape[1]  , hf[regime+'/data'].shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator without shuffling\n",
    "# yields (data, labels, adjacency)\n",
    "class generator_no_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data'].shape[0]\n",
    "            self.data_length = 32  # hf[self.regime+'/data'].shape[1]   # поставлю\n",
    "        self.batch_num = self.num // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = self.batch_num*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = 0\n",
    "        stop = self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            for i in range(self.batch_num):\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                   self.full_adj*mask_to_adj ) #labels,\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                   self.full_adj*mask_to_adj ) #labels,\n",
    "                \n",
    "# generator with shuffling\n",
    "class generator_with_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, buffer_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        self.buffer_size = buffer_size\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data/'].shape[0]\n",
    "            self.data_length = 32 #hf[self.regime+'/data'].shape[1]\n",
    "        self.batch_num = (self.num-self.buffer_size) // self.batch_size\n",
    "        self.last_batches_num = self.buffer_size // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = (self.batch_num+self.last_batches_num)*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = self.buffer_size\n",
    "        stop = self.buffer_size + self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            mask = hf[self.regime+'/mask'][:self.buffer_size]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "            buffer_data = np.concatenate((hf[self.regime+'/data'][:self.buffer_size],mask_channel), axis=-1)\n",
    "            #buffer_labels = hf[self.regime+'/labels_signal_noise'][:self.buffer_size]\n",
    "            buffer_adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            for i in range(self.batch_num):\n",
    "                idxs = rd.sample( range(self.buffer_size), k=self.batch_size )\n",
    "                yield ( buffer_data[idxs],  buffer_adj[idxs] )\n",
    "                #yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                buffer_data[idxs] = np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)\n",
    "                #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                #buffer_labels[idxs] = labels\n",
    "                adj = self.full_adj*mask_channel*mask_channel_2\n",
    "                buffer_adj[idxs] = adj\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            # fill the buffer with left data, if any\n",
    "            mask = hf[self.regime+'/mask'][start:stop]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "            buffer_data = np.concatenate( (buffer_data,np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel),axis=-1)), axis=0 )\n",
    "            #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "            #buffer_labels = np.concatenate( (buffer_labels,labels), axis=0 )\n",
    "            adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            buffer_adj = np.concatenate( (buffer_adj,adj), axis=0 )\n",
    "            #sh_idxs = rd.sample( range(buffer_labels.shape[0]), k=buffer_labels.shape[0] )\n",
    "            start = 0\n",
    "            stop = self.batch_size\n",
    "            for i in range(self.last_batches_num):\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                #yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                yield ( buffer_data[idxs], buffer_adj[idxs] )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs],  buffer_adj[idxs] )\n",
    "               # yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c876a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "def make_datasets(h5f,make_generator_shuffle,return_batch_reminder,train_batch_size,train_buffer_size,test_batch_size,k_nearest):\n",
    "    # generator for training data\n",
    "    if make_generator_shuffle:\n",
    "        tr_generator = generator_with_shuffle(h5f,'train',train_batch_size,train_buffer_size,return_batch_reminder,k_nearest)\n",
    "    else:\n",
    "        tr_generator = generator_no_shuffle(h5f,'train',train_batch_size,return_batch_reminder,k_nearest)\n",
    "    if return_batch_reminder:\n",
    "        # size of the last batch is unknown\n",
    "        tr_batch_size = None\n",
    "    else:\n",
    "        tr_batch_size = train_batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator( tr_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                         tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "\n",
    "    if make_generator_shuffle:\n",
    "        train_dataset = train_dataset.repeat(-1).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_dataset = train_dataset.repeat(-1).shuffle(num_batch_shuffle)\n",
    "\n",
    "    # generator for validation data\n",
    "    te_generator = generator_no_shuffle(h5f,'test',test_batch_size,False,k_nearest)\n",
    "    te_batch_size = tr_batch_size\n",
    "    test_dataset = tf.data.Dataset.from_generator( te_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                          tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "    \n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc9f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:06:20.620746: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 17:06:21.121019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10410 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = make_datasets(h5f,True,False,batch_size,500*batch_size,batch_size,k_nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4098b",
   "metadata": {},
   "source": [
    "### Graph Convolutional Model\n",
    "\n",
    "This is simplest graph neural network. The protocol goes as follows:\n",
    "\n",
    "1) Transform initial encoding of the nodes (NodesEncoder). This is usually one-layer NN.\n",
    "\n",
    "2) Update nodes encodings by agrregating information from connected cells (StateUpdater). Simple NN or just linear algebra.\n",
    "\n",
    "In the model, we normilize adjacency matrix to have a better data flow.\n",
    "\n",
    "The dataset yields element of the form (data, albels, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f7a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-transforms nodes\n",
    "# adds res connection for better data flow\n",
    "class NodesEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation):\n",
    "        super(NodesEncoder, self).__init__()\n",
    "        self.num_layers = len(units)\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(un) for un in units ]\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        if self.num_layers==0:\n",
    "            self.out_encs_length = input_shape[-1]\n",
    "        else:\n",
    "            self.out_encs_length = self.units[-1]+input_shape[-1]\n",
    "\n",
    "    def call(self, x):\n",
    "        init_x = x\n",
    "        for lr in self.dense_layers:\n",
    "            x = lr(x)\n",
    "            x = self.activation(x)\n",
    "            x = tf.concat((x,init_x),axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd63a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states updater\n",
    "class StateUpdater(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation):\n",
    "        super(StateUpdater, self).__init__()\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(un) for un in units ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, vert, adj):\n",
    "        # aggragate information from neighbours\n",
    "        vert = tf.matmul(adj, vert)\n",
    "        # update state\n",
    "        for lr in self.dense_layers:\n",
    "            vert = lr(vert)\n",
    "            vert = self.activation(vert)\n",
    "        return vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes_encoder, state_updater):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.nodes_encoder = nodes_encoder\n",
    "        self.state_updater = state_updater\n",
    "        \n",
    "    def call(self, vert, adj):\n",
    "        # transform features\n",
    "        transf_encs = self.nodes_encoder(vert)\n",
    "        # update states\n",
    "        new_encs = self.state_updater(transf_encs, adj)\n",
    "        return new_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc6980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvModel(tf.keras.Model):\n",
    "    \n",
    "    # last graph layer must have 2 chennels to apply softmax \n",
    "    def __init__(self, graph_conv_layers):\n",
    "        super(GraphConvModel, self).__init__()\n",
    "        self.gr_layers = [gr for gr in graph_conv_layers]\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn, metrics):\n",
    "        super(GraphConvModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.metrics_ = metrics\n",
    "        self.all_metrics = metrics+[self.loss_tracker]\n",
    "        \n",
    "    def norm_adj(self, adj):\n",
    "        degree = tf.reduce_sum(adj, axis=-1)\n",
    "        # small plus to avoid 0/0\n",
    "        norm_degree = tf.linalg.diag(1./tf.sqrt(degree+1e-8))\n",
    "        n_adj = tf.matmul(norm_degree, tf.matmul(adj, norm_degree) )\n",
    "        return n_adj\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        adj = self.norm_adj(adj)\n",
    "        mask = x[:,:,-1:]\n",
    "        for gr_lr in self.gr_layers:\n",
    "            x = gr_lr(x, adj)\n",
    "        # yield correct predictions for auxillary hits\n",
    "        preds = tf.where( tf.cast(mask,bool), x, tf.constant([0.,1.]) )\n",
    "        return preds\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.all_metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.call(datas)\n",
    "            loss = self.loss_fn(labels,preds)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients( zip(grads, self.trainable_weights) )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        preds = self.call(datas)\n",
    "        loss = self.loss_fn(labels,preds)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3354165",
   "metadata": {},
   "outputs": [],
   "source": [
    "selu = tf.keras.activations.selu\n",
    "softmax = tf.keras.activations.softmax\n",
    "\n",
    "nodes_encs_length_s = [[16],[16]]\n",
    "new_state_length_s = [[32],[2]]\n",
    "state_upd_acts = [selu,softmax]\n",
    "\n",
    "gr_layers = []\n",
    "for (nodes_l, news_l, new_act) in zip(nodes_encs_length_s,new_state_length_s, state_upd_acts):\n",
    "    nodes_encoder = NodesEncoder(nodes_l, selu)\n",
    "    state_updater = StateUpdater(news_l, new_act)\n",
    "    gr_layers.append( GraphConvLayer(nodes_encoder,state_updater) )\n",
    "    \n",
    "gnn = GraphConvModel(gr_layers)\n",
    "\n",
    "lr = 0.002\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "gnn.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e7719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 170s 67ms/step - accuracy: 0.9728 - loss: 0.0664 - val_accuracy: 0.9716 - val_loss: 0.0679\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 166s 66ms/step - accuracy: 0.9755 - loss: 0.0608 - val_accuracy: 0.9728 - val_loss: 0.0651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f626eab00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d6dad",
   "metadata": {},
   "source": [
    "### Graph Attention Model\n",
    "\n",
    "Some of the neighbouring nodes are irrelevant, we need to account for this fact. \n",
    "\n",
    "We do this by introducing NN for establishing relevance of two nodes (AttentionEstablisher). It weights the contributions of nodes as seen by the state updater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1013cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates attentions for pair of channels\n",
    "class AttentionEstablisher(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hid_units, activation):\n",
    "        super(AttentionEstablisher, self).__init__()\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(un) for un in hid_units ]\n",
    "        self.activation = activation\n",
    "        # use softmax, not sigmoid - better data flow\n",
    "        self.soft_layer = tf.keras.layers.Dense(1, activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        for lr in self.dense_layers:\n",
    "            x = lr(x)\n",
    "            x = self.activation(x)\n",
    "        res = self.soft_layer(x)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b73293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes_encoder, state_updater, attention_establisher):\n",
    "        super(GraphAttLayer, self).__init__()\n",
    "        self.nodes_encoder = nodes_encoder\n",
    "        self.attention_establisher = attention_establisher\n",
    "        self.state_updater = state_updater\n",
    "\n",
    "    def make_pairs(self, vert, adj):\n",
    "        adj_exp = tf.expand_dims(adj,axis=-1)\n",
    "        vert_exp_1 = tf.expand_dims(vert,axis=1) # (bs,1,om,c)\n",
    "        vert_exp_2 = tf.expand_dims(vert,axis=2) # (bs,om,1,c)\n",
    "        rel_vert = adj_exp*vert_exp_2 # (bs, om, om_targ, c)\n",
    "        base_vert = adj_exp*vert_exp_1 # (bs, om_base, om, c)\n",
    "        pairs = tf.concat((rel_vert,base_vert),axis=-1) # (bs, om_base, om_target, 2c)\n",
    "        return pairs\n",
    "        \n",
    "    def call(self, vert, adj):\n",
    "        # transform features\n",
    "        transf_encs = self.nodes_encoder(vert)\n",
    "        # calculate attentions\n",
    "        mask_adj = tf.cast(adj, bool)\n",
    "        pairs = self.make_pairs(vert, adj)\n",
    "        ## making loop over OMs might be better in terms of memory and calculations efficiency\n",
    "        atts = tf.where(mask_adj, tf.squeeze(self.attention_establisher(pairs)), 0.)\n",
    "        # aggregate information, use attention weights instead of adj matrix\n",
    "        new_encs = self.state_updater(transf_encs, atts)\n",
    "        return new_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d3b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we do not need to normilize adj matrix\n",
    "class GraphAttModel(tf.keras.Model):\n",
    "    \n",
    "    # last graph layer must have 2 chennels to apply softmax \n",
    "    def __init__(self, graph_layers):\n",
    "        super(GraphAttModel, self).__init__()\n",
    "        self.gr_layers = [gr for gr in graph_layers]\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn, metrics):\n",
    "        super(GraphAttModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.metrics_ = metrics\n",
    "        self.all_metrics = metrics+[self.loss_tracker]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        mask = x[:,:,-1:]\n",
    "        for gr_lr in self.gr_layers:\n",
    "            x = gr_lr(x, adj)\n",
    "        # yield correct predictions for auxillary hits\n",
    "        preds = tf.where( tf.cast(mask,bool), x, tf.constant([0.,1.]) )\n",
    "        return preds\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.all_metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.call(datas)\n",
    "            loss = self.loss_fn(labels,preds)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients( zip(grads, self.trainable_weights) )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        preds = self.call(datas)\n",
    "        loss = self.loss_fn(labels,preds)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bdcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selu = tf.keras.activations.selu\n",
    "softmax = tf.keras.activations.softmax\n",
    "\n",
    "nodes_encs_length_s = [[16],[16]]\n",
    "new_state_length_s = [[32],[2]]\n",
    "att_units_s = [[16],[16]]\n",
    "state_upd_acts = [selu,softmax]\n",
    "att_upd_acts = [selu,selu]\n",
    "\n",
    "gr_layers = []\n",
    "for (nodes_l,news_l,new_act,att_l,att_act) in zip(nodes_encs_length_s,new_state_length_s,state_upd_acts,att_units_s,att_upd_acts):\n",
    "    nodes_encoder = NodesEncoder(nodes_l, selu)\n",
    "    state_updater = StateUpdater(news_l, new_act)\n",
    "    attention_establisher = AttentionEstablisher(att_l, att_act)\n",
    "    gr_layers.append( GraphAttLayer(nodes_encoder,state_updater,attention_establisher) )\n",
    "    \n",
    "gnn = GraphAttModel(gr_layers)\n",
    "\n",
    "lr = 0.002\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "gnn.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baae8d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2500/2500 [==============================] - 169s 67ms/step - accuracy: 0.9726 - loss: 0.0697 - val_accuracy: 0.9714 - val_loss: 0.0681\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 163s 65ms/step - accuracy: 0.9755 - loss: 0.0610 - val_accuracy: 0.9728 - val_loss: 0.0650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f00229870>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05eb7e",
   "metadata": {},
   "source": [
    "### Message Passing Graph\n",
    "\n",
    "Verticies keep information on themselves. Hence their encoding are not optimal for updating encodings of other verticies. It makes sence then to prepare messages from one node to enother used for updating states (MessageCreator). \n",
    "\n",
    "In this cases attention is not needed as messages should incorporate relevance.\n",
    "\n",
    "State updater should be modified to use messages (does not need adj matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67901a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states updater MP\n",
    "class StateUpdaterMP(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation):\n",
    "        super(StateUpdaterMP, self).__init__()\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(un) for un in units ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, vert):\n",
    "        # update state\n",
    "        for lr in self.dense_layers:\n",
    "            vert = lr(vert)\n",
    "            vert = self.activation(vert)\n",
    "        return vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88b9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates messages for pairs of nodes\n",
    "class MessageCreator(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hid_units, activation):\n",
    "        super(MessageCreator, self).__init__()\n",
    "        self.dense_layers = [ tf.keras.layers.Dense(un) for un in hid_units ]\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, x):\n",
    "        for lr in self.dense_layers:\n",
    "            x = lr(x)\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba3eb358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMessPassLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes_encoder, state_updater, message_creator):\n",
    "        super(GraphMessPassLayer, self).__init__()\n",
    "        self.nodes_encoder = nodes_encoder\n",
    "        self.message_creator = message_creator\n",
    "        self.state_updater = state_updater\n",
    "\n",
    "    def make_pairs(self, vert, adj):\n",
    "        adj_exp = tf.expand_dims(adj,axis=-1)\n",
    "        vert_exp_1 = tf.expand_dims(vert,axis=1) # (bs,1,om,c)\n",
    "        vert_exp_2 = tf.expand_dims(vert,axis=2) # (bs,om,1,c)\n",
    "        rel_vert = adj_exp*vert_exp_2 # (bs, om, om_targ, c)\n",
    "        base_vert = adj_exp*vert_exp_1 # (bs, om_base, om, c)\n",
    "        pairs = tf.concat((rel_vert,base_vert),axis=-1) # (bs, om_base, om_target, 2c)\n",
    "        return pairs\n",
    "        \n",
    "    def call(self, vert, adj):\n",
    "        # transform features\n",
    "        transf_encs = self.nodes_encoder(vert)\n",
    "        mask_adj = tf.expand_dims( tf.cast(adj, bool), axis=-1 )\n",
    "        pairs = self.make_pairs(vert, adj)\n",
    "        ## making loop over OMs might be better in terms of memory and calculations efficiency\n",
    "        messages = tf.where(mask_adj, self.message_creator(pairs), [0.]) # (bs, om, om, c)\n",
    "        # take mean over target OMs to form message\n",
    "        n_neigh = tf.expand_dims( tf.reduce_sum( adj,axis=1 ), axis=-1 )\n",
    "        messages = tf.math.reduce_sum(messages, axis=2, keepdims=False)/(n_neigh+1e-8)\n",
    "        # concat message and current encodings\n",
    "        upd_from = tf.concat((transf_encs,messages), axis=-1)\n",
    "        # update state\n",
    "        new_encs = self.state_updater(upd_from)\n",
    "        return new_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a08c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we do not need to normilize adj matrix\n",
    "class GraphMPModel(tf.keras.Model):\n",
    "    \n",
    "    # last graph layer must have 2 chennels to apply softmax \n",
    "    def __init__(self, graph_layers):\n",
    "        super(GraphMPModel, self).__init__()\n",
    "        self.gr_layers = [gr for gr in graph_layers]\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn, metrics):\n",
    "        super(GraphMPModel, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.metrics_ = metrics\n",
    "        self.all_metrics = metrics+[self.loss_tracker]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        mask = x[:,:,-1:]\n",
    "        for gr_lr in self.gr_layers:\n",
    "            x = gr_lr(x, adj)\n",
    "        # yield correct predictions for auxillary hits\n",
    "        preds = tf.where( tf.cast(mask,bool), x, tf.constant([0.,1.]) )\n",
    "        return preds\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.all_metrics\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.call(datas)\n",
    "            loss = self.loss_fn(labels,preds)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients( zip(grads, self.trainable_weights) )\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, datas):\n",
    "        (x, labels, adj) = datas\n",
    "        preds = self.call(datas)\n",
    "        loss = self.loss_fn(labels,preds)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        prd_cls = tf.math.argmax( preds, axis=-1 )\n",
    "        true_cls = tf.math.argmax( labels, axis=-1 )\n",
    "        for m in self.metrics_:\n",
    "            m.update_state(prd_cls, true_cls)\n",
    "        ms = { m.name : m.result() for m in self.all_metrics }\n",
    "        return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9dd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selu = tf.keras.activations.selu\n",
    "softmax = tf.keras.activations.softmax\n",
    "\n",
    "nodes_encs_length_s = [[16],[16]]\n",
    "new_state_length_s = [[32],[2]]\n",
    "mess_units_s = [[16],[16]]\n",
    "state_upd_acts = [selu,softmax]\n",
    "mess_acts = [selu,selu]\n",
    "\n",
    "gr_layers = []\n",
    "for (nodes_l,news_l,new_act,mess_l,mess_act) in zip(nodes_encs_length_s,new_state_length_s,state_upd_acts,mess_units_s,mess_acts):\n",
    "    nodes_encoder = NodesEncoder(nodes_l, selu)\n",
    "    state_updater = StateUpdaterMP(news_l, new_act)\n",
    "    message_creator = MessageCreator(mess_l, mess_act)\n",
    "    gr_layers.append( GraphMessPassLayer(nodes_encoder,state_updater,message_creator) )\n",
    "    \n",
    "gnn = GraphAttModel(gr_layers)\n",
    "\n",
    "lr = 0.002\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "gnn.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ce93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:17:31.239158: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] layout failed: INVALID_ARGUMENT: Size of values 1 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/StatefulPartitionedCall/graph_mess_pass_layer/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - ETA: 0s - accuracy: 0.9798 - loss: 0.0527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:20:03.422661: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:828] layout failed: INVALID_ARGUMENT: Size of values 1 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/StatefulPartitionedCall/graph_mess_pass_layer/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 163s 65ms/step - accuracy: 0.9798 - loss: 0.0527 - val_accuracy: 0.9827 - val_loss: 0.0442\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 162s 65ms/step - accuracy: 0.9845 - loss: 0.0418 - val_accuracy: 0.9846 - val_loss: 0.0398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ef86bc310>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn.fit(train_dataset, steps_per_epoch=2500, validation_steps=500, epochs=2, validation_data=test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff8d54",
   "metadata": {},
   "source": [
    "### Possible improvements.\n",
    "\n",
    "1) Usually, GNN should be agnostic to vetricies labeling (hence taking mean). In our case, however, we have a very good choice of natural ordering - according to activation times. We can make use of this ordering by making OM-wise convolutions for nodes transformations.\n",
    "\n",
    "2) Introduce edge features.This might be usefull for track segmentation.\n",
    "\n",
    "3) Introduce master vertex. It stores information about the graph as a whole. At each iteration, it can be used to update vertx encoding and vice versa.\n",
    "\n",
    "4) Make adj matrix dynamic (like edge feature). That is, all vetricies are connected and elements in adj matrix define the weight of the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba682195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator without shuffling\n",
    "# yields (data, labels, adjacency)\n",
    "class generator_no_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data'].shape[0]\n",
    "            self.data_length = 32  # hf[self.regime+'/data'].shape[1]   # поставлю\n",
    "        self.batch_num = self.num // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = self.batch_num*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = 0\n",
    "        stop = self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            for i in range(self.batch_num):\n",
    "                #mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask = hf[self.regime+'/mask'][start:stop,:self.data_length]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                polar = hf[self.regime+'/ev_chars'][start:stop]\n",
    "                cos, sin = np.expand_dims(np.cos(polar),axis=1) ,  np.expand_dims(np.sin(polar),axis=1)\n",
    "                target = np.concatenate((cos, sin), axis=-1)\n",
    "                #yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1), \n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop,:self.data_length], mask_channel), axis=-1), \n",
    "                   target, self.full_adj*mask_to_adj ) #labels,\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                #mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask = hf[self.regime+'/mask'][start:stop,:self.data_length]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                mask_to_adj = mask_channel*mask_channel_2\n",
    "                #labels = hf[self.regime+'/labels_signal_noise'][start:stop]\n",
    "                polar = hf[self.regime+'/ev_chars'][start:stop]\n",
    "                cos, sin = np.expand_dims(np.cos(polar),axis=1) ,  np.expand_dims(np.sin(polar),axis=1)\n",
    "                target = np.concatenate((cos, sin), axis=-1)\n",
    "                #yield ( np.concatenate((hf[self.regime+'/data'][start:stop],mask_channel), axis=-1),\n",
    "                yield ( np.concatenate((hf[self.regime+'/data'][start:stop,:self.data_length], mask_channel)\n",
    "                   axis=-1), target, self.full_adj*mask_to_adj ) #labels,\n",
    "                \n",
    "# generator with shuffling\n",
    "class generator_with_shuffle:\n",
    "    \n",
    "    def __init__(self, file, regime, batch_size, buffer_size, return_reminder, k_nearest):\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.return_reminder = return_reminder\n",
    "        self.buffer_size = buffer_size\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            self.num = hf[self.regime+'/data/'].shape[0]\n",
    "            self.data_length = 32 #hf[self.regime+'/data'].shape[1]\n",
    "        self.batch_num = (self.num-self.buffer_size) // self.batch_size\n",
    "        self.last_batches_num = self.buffer_size // self.batch_size\n",
    "        if return_reminder:\n",
    "            self.gen_num = self.num\n",
    "        else:\n",
    "            self.gen_num = (self.batch_num+self.last_batches_num)*self.batch_size\n",
    "        te = [ np.expand_dims(np.eye(self.data_length),axis=0)]\n",
    "        for i in range(1,k_nearest):\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=i),axis=0))\n",
    "            te.append(np.expand_dims(np.eye(self.data_length, k=-i),axis=0))\n",
    "        self.full_adj = np.sum( np.concatenate(te, axis=0), axis=0 )\n",
    "\n",
    "    def __call__(self):\n",
    "        start = self.buffer_size\n",
    "        stop = self.buffer_size + self.batch_size\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            mask = hf[self.regime+'/mask'][:self.buffer_size,:self.data_length]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "            buffer_data = np.concatenate((hf[self.regime+'/data'][:self.buffer_size,:self.data_length],mask_channel),\n",
    "                                         axis=-1)\n",
    "            polar = hf[self.regime+'/ev_chars'][:self.buffer_size]\n",
    "            cos, sin = np.expand_dims(np.cos(polar),axis=1) ,  np.expand_dims(np.sin(polar),axis=1)\n",
    "            buffer_target = np.concatenate((cos, sin), axis=-1)            \n",
    "\n",
    "            buffer_adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            for i in range(self.batch_num):\n",
    "                idxs = rd.sample( range(self.buffer_size), k=self.batch_size )\n",
    "                yield ( buffer_data[idxs], buffer_target[idxs]   buffer_adj[idxs] )\n",
    "                #yield ( buffer_data[idxs], buffer_labels[idxs], buffer_adj[idxs] )\n",
    "                #mask = hf[self.regime+'/mask'][start:stop]\n",
    "                mask = hf[self.regime+'/mask'][start:stop,:self.data_length]\n",
    "                mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "                mask_channel_2 = np.expand_dims( mask, axis=1 )\n",
    "                buffer_data[idxs] = np.concatenate((hf[self.regime+'/data'][start:stop,:self.data_length],mask_channel),\n",
    "                                                   axis=-1)\n",
    "                polar = hf[self.regime+'/ev_chars'][start:stop]\n",
    "                cos, sin = np.expand_dims(np.cos(polar),axis=1) ,  np.expand_dims(np.sin(polar),axis=1)\n",
    "                target = np.concatenate((cos, sin), axis=-1)  \n",
    "                buffer_target[idx] = target\n",
    "                adj = self.full_adj*mask_channel*mask_channel_2\n",
    "                buffer_adj[idxs] = adj\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            # fill the buffer with left data, if any\n",
    "            #mask = hf[self.regime+'/mask'][start:stop]\n",
    "            mask = hf[self.regime+'/mask'][start:stop,:self.data_length]\n",
    "            mask_channel = np.expand_dims( mask, axis=-1 )\n",
    "            mask_channel_2 = np.expand_dims( mask, axis=1 )     \n",
    "            buffer_data = np.concatenate( (buffer_data,np.concatenate((hf[self.regime+'/data'][start:stop,:self.data_length],mask_channel),\n",
    "                                                                      axis=-1)), axis=0 )\n",
    "            polar = hf[self.regime+'/ev_chars'][start:stop]\n",
    "            cos, sin = np.expand_dims(np.cos(polar),axis=1) ,  np.expand_dims(np.sin(polar),axis=1)\n",
    "            target = np.concatenate((cos, sin), axis=-1)  \n",
    "            buffer_target = np.concatenate( (buffer_target,target), axis=0 )\n",
    "            adj = self.full_adj*mask_channel*mask_channel_2\n",
    "            buffer_adj = np.concatenate( (buffer_adj,adj), axis=0 )\n",
    "            sh_idxs = rd.sample( range(buffer_target.shape[0]), k=buffer_target.shape[0] )\n",
    "            start = 0\n",
    "            stop = self.batch_size\n",
    "            for i in range(self.last_batches_num):\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_target[idxs], buffer_adj[idxs] )\n",
    "                start += self.batch_size\n",
    "                stop += self.batch_size\n",
    "            if self.return_reminder:\n",
    "                idxs = sh_idxs[start:stop]\n",
    "                yield ( buffer_data[idxs], buffer_target[idxs], buffer_adj[idxs] )\n",
    "                \n",
    "### Datasets\n",
    "def make_datasets(h5f,make_generator_shuffle,return_batch_reminder,train_batch_size,train_buffer_size,test_batch_size,k_nearest):\n",
    "    # generator for training data\n",
    "    if make_generator_shuffle:\n",
    "        tr_generator = generator_with_shuffle(h5f,'train',train_batch_size,train_buffer_size,return_batch_reminder,k_nearest)\n",
    "    else:\n",
    "        tr_generator = generator_no_shuffle(h5f,'train',train_batch_size,return_batch_reminder,k_nearest)\n",
    "    if return_batch_reminder:\n",
    "        # size of the last batch is unknown\n",
    "        tr_batch_size = None\n",
    "    else:\n",
    "        tr_batch_size = train_batch_size\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! здесь изменить\n",
    "    train_dataset = tf.data.Dataset.from_generator( tr_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                         tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "\n",
    "    if make_generator_shuffle:\n",
    "        train_dataset = train_dataset.repeat(-1).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_dataset = train_dataset.repeat(-1).shuffle(num_batch_shuffle)\n",
    "\n",
    "    # generator for validation data\n",
    "    te_generator = generator_no_shuffle(h5f,'test',test_batch_size,False,k_nearest)\n",
    "    te_batch_size = tr_batch_size\n",
    "    test_dataset = tf.data.Dataset.from_generator( te_generator, \n",
    "                        output_signature=( tf.TensorSpec(shape=(tr_batch_size,max_len,6)), tf.TensorSpec(shape=(tr_batch_size,max_len,2)),\n",
    "                                          tf.TensorSpec(shape=(tr_batch_size,max_len,max_len))) )\n",
    "    \n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
