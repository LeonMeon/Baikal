{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafe8da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# makes bundle data\n",
    "# collects energy, angles, flags, event ids\n",
    "#########################\n",
    "\n",
    "import numpy as np\n",
    "import uproot as ur\n",
    "import h5py as h5\n",
    "import os\n",
    "\n",
    "# cuts on number of signal hits \n",
    "cuts_signal = [8]\n",
    "# output file name\n",
    "h5_name = 'mc_baikal_bundle_pure_string-cut_v4_8OM_mu.h5'\n",
    "\n",
    "# prefixes for files\n",
    "particles = ['mu'] #,'nu'\n",
    "\n",
    "# pathes in .root file to registered charges, times, and event flags.\n",
    "pathes_data = ['Events/BEvent./BEvent.fPulses/BEvent.fPulses.fAmplitude', 'Events/BEvent./BEvent.fPulses/BEvent.fPulses.fTime', \n",
    "                'Events/MCEventMask./MCEventMask.BEventMask/MCEventMask.BEventMask.fOrigins/MCEventMask.BEventMask.fOrigins.fFlag']\n",
    "\n",
    "# pathes to event id, angles, energy\n",
    "pathes_ev_chars = ['Events/BJointHeader./BJointHeader.fEventID',\n",
    "                'Events/BMCEvent./BMCEvent.fPrimaryParticlePolar', 'Events/BMCEvent./BMCEvent.fPrimaryParticleAzimuth',\n",
    "                'Events/BMCEvent./BMCEvent.fPrimaryParticleEnergy']\n",
    "\n",
    "# pathes to geometry data\n",
    "pathes_geometry = 'ArrayConfig/BGeomTel./BGeomTel.BGeomTel/BGeomTel.BGeomTel.fOMs/BGeomTel.BGeomTel.fOMs.fPosition'\n",
    "\n",
    "# dict for mapping string numbers to grid idxs\n",
    "map_dict = {}\n",
    "map_dict[6] = (0,0)\n",
    "map_dict[0] = (0,1)\n",
    "map_dict[1] = (0,2)\n",
    "map_dict[5] = (1,0)\n",
    "map_dict[7] = (1,1)\n",
    "map_dict[2] = (1,2)\n",
    "map_dict[4] = (2,0)\n",
    "map_dict[3] = (2,1)\n",
    "\n",
    "# routine for making bundle\n",
    "def make_bundle(tr_channeles,data):\n",
    "    # order data according to the registered charge\n",
    "    # in this case for the channels appearing twice recorded is the time with largest signal\n",
    "    sort_idxs = [ np.argsort(c,axis=0) for c in data[0] ]\n",
    "    sorted_data = [ np.array([d[idx] for d,idx in zip(dt,sort_idxs)], dtype=object) for dt in data ]\n",
    "    tr_channeles = np.array([d[idx] for d,idx in zip(tr_channeles,sort_idxs)], dtype=object) \n",
    "    # get grid idxs\n",
    "    n_layer = [ [ ch % 36 for ch in channles ] for channles in tr_channeles ]\n",
    "    n_string = [ [ ch // 36 for ch in channles ] for channles in tr_channeles ]\n",
    "    n_grid = [ [ map_dict[n] for n in ns ] for ns in n_string ]\n",
    "    coords = [ [ (z,x,y) for (z,(x,y)) in zip(zs,ss) ] for (zs,ss) in zip(n_layer,n_grid) ]\n",
    "    # make bundle\n",
    "    # data, mask, and labels seperatly\n",
    "    bundle = np.zeros( (len(data[0]),36,3,3,2), dtype=np.float32 )\n",
    "    mask_bundle = np.full( (len(data[0]),36,3,3), False )\n",
    "    labels_bundle = np.zeros( (len(data[0]),36,3,3), dtype=np.float32 )\n",
    "    for (m,l,b,chs,ts,fs,cs) in zip(mask_bundle,labels_bundle,bundle,sorted_data[0],sorted_data[1],sorted_data[2],coords):\n",
    "        for (c,ch,t,f) in zip(cs,chs,ts,fs):\n",
    "            m[c] = True\n",
    "            l[c] = f\n",
    "            b[c][0] = ch\n",
    "            b[c][1] = t\n",
    "    return (bundle, mask_bundle, labels_bundle)\n",
    "\n",
    "# loop over particles -> files\n",
    "for particle in particles:\n",
    "    root_file_entries = os.scandir('/net/63/home/ivkhar/Baikal/data/MC/'+particle+'atm/') #/home/ivkhar/Baikal/data/MC/\n",
    "    # loop over files\n",
    "    file_num = 0\n",
    "    for root_file in root_file_entries:\n",
    "        id_prefix = root_file.name[:2]+'_'+root_file.name[-9:-5]+'_'\n",
    "        with ur.open(root_file.path) as rf: \n",
    "            # read coordinates of the detectors\n",
    "            # reading as awkward array and casting to numpy is faster for big arrays\n",
    "            # yes it takes much memory during reading\n",
    "            # in MC, only one entry\n",
    "            coordinates_awk = rf[pathes_geometry].array( )\n",
    "            coordinates = np.zeros((len(coordinates_awk[0]),3), dtype=np.float32)\n",
    "            coordinates[:,0] = coordinates_awk['fX']\n",
    "            coordinates[:,1] = coordinates_awk['fY']\n",
    "            coordinates[:,2] = coordinates_awk['fZ']\n",
    "            re_coordinates = np.reshape( coordinates, (36,8,3), order='F' )\n",
    "            positions = np.zeros( (36,3,3,3), dtype=np.float32 )\n",
    "            idxs = [map_dict[n] for n in range(8)]\n",
    "            for p,cs in zip(positions,re_coordinates):\n",
    "                for (idx,c) in zip(idxs,cs):\n",
    "                    p[idx] = c\n",
    "            # read data: channels, charges, times, flags\n",
    "            data = [ rf[p].array(library=\"np\") for p in pathes_data ]\n",
    "            # get x, y, z as posiitons in cluster\n",
    "            tr_channeles = rf['Events/BEvent./BEvent.fPulses/BEvent.fPulses.fChannelID'].array(library=\"np\")\n",
    "            # get idxs of signal hits\n",
    "            flags = data[2]\n",
    "            signals = [ np.nonzero(f!=0) for f in flags ]\n",
    "            clear_data = [ [d[s] for (d,s) in zip(dd,signals)] for dd in data ]\n",
    "            clear_tr_ch = [ d[s] for (d,s) in zip(tr_channeles,signals) ]\n",
    "            # make bundle\n",
    "            (bundle_data, bundle_mask, bundle_label)  = make_bundle(clear_tr_ch,clear_data)\n",
    "            # get number of strings\n",
    "            sig_channels = [ c[s] for c,s in zip(tr_channeles,signals) ]\n",
    "            sig_strings = [ [c // 36 for c in ch] for ch in sig_channels ]\n",
    "            un_strings = [ set(s) for s in sig_strings ]\n",
    "            num_un_strings = np.array([ float(len(s)) for s in un_strings ])\n",
    "            # get events characteristics          \n",
    "            ev_chars = np.array([ rf[p].array(library=\"np\") for p in pathes_ev_chars ])\n",
    "            ev_chars = np.transpose( ev_chars, (1,0) )\n",
    "            ev_chars = np.concatenate( [ev_chars,np.expand_dims(num_un_strings, axis=-1)], axis=-1 )\n",
    "            #ev_chars = np.array([ ch for (ch,pp) in zip(ev_chars,pass_string_cut_events) if pp ])\n",
    "            # impose cut on active channels and strings\n",
    "            bi_labels = np.where( bundle_label!=0, 1, 0 )\n",
    "            num_signal = np.sum( bi_labels, axis=(1,2,3) )\n",
    "            for cut in cuts_signal:\n",
    "                pass_idxs = np.nonzero( np.logical_and(num_signal>=cut,num_un_strings>1) )\n",
    "                pass_bundle_data = bundle_data[pass_idxs]\n",
    "                pass_bundle_label = bundle_label[pass_idxs]\n",
    "                pass_bundle_mask = bundle_mask[pass_idxs]\n",
    "                pass_bundle_ev_chars = ev_chars[pass_idxs][...,1:]\n",
    "                pass_bundle_ev_ids = ev_chars[pass_idxs][...,0].tolist()\n",
    "                pass_bundle_ev_ids = np.array([ id_prefix+str(int(ev_id)) for ev_id in pass_bundle_ev_ids ]).astype(np.string_)             \n",
    "                # write data to file\n",
    "                with h5.File('/home/leonov/Baikal/Cut_8_mu/Data/'+h5_name,'a') as hf:\n",
    "                    hf.create_dataset('data/cut_'+str(cut)+'/'+particle+'/part_'+str(file_num)+'/data', data=pass_bundle_data, dtype='float32', compression='gzip', compression_opts=9)         \n",
    "                    hf.create_dataset('labels/cut_'+str(cut)+'/'+particle+'/part_'+str(file_num)+'/data', data=pass_bundle_label, dtype='float32')\n",
    "                    hf.create_dataset('ev_ids/cut_'+str(cut)+'/'+particle+'/part_'+str(file_num)+'/data', data=pass_bundle_ev_ids)\n",
    "                    hf.create_dataset('ev_chars/cut_'+str(cut)+'/'+particle+'/part_'+str(file_num)+'/data', data=pass_bundle_ev_chars)\n",
    "                    hf.create_dataset('mask/cut_'+str(cut)+'/'+particle+'/part_'+str(file_num)+'/data', data=pass_bundle_mask.astype(bool), dtype=bool)\n",
    "            file_num += 1\n",
    "# write coordinates\n",
    "# assumed that they are fixed\n",
    "with h5.File('/home/leonov/Baikal/Cut_8_mu/Data/'+h5_name,'a') as hf:\n",
    "    hf.create_dataset('geometry/bundle_coordinates', data=positions, dtype='float32')\n",
    "print(file_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615a7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "cuts_signal = [8]\n",
    "h5f = 'mc_baikal_bundle_pure_string-cut_v4_8OM_mu.h5'\n",
    "use_postfix = False # True\n",
    "\n",
    "particles = ['mu'] #'mu',\n",
    "num_files_to_use = {'mu':1990} #'mu':0,\n",
    "if use_postfix:\n",
    "    postfix = '_mu-'+str(num_files_to_use['mu'])+'_nu-'+str(num_files_to_use['nu'])\n",
    "else:\n",
    "    postfix = ''\n",
    "\n",
    "with h5.File('/home/leonov/Baikal/Cut_8_mu/Data/'+h5f,'a') as hf:\n",
    "    for cut in cuts_signal:\n",
    "        # get numbers of events\n",
    "        nums_dict = {}\n",
    "        parts_dict = {}\n",
    "        for particle in particles:\n",
    "            nums_dict[particle] = {}\n",
    "            parts = list(hf['data/cut_'+str(cut)+'/'+particle].keys())\n",
    "            parts_dict[particle] = parts[:num_files_to_use[particle]]    \n",
    "            for part in parts_dict[particle]:\n",
    "                nums_dict[particle][part] = hf['data/cut_'+str(cut)+'/'+particle+'/'+part+'/data'].shape[0]\n",
    "        # get local avg\n",
    "        means = []\n",
    "        nums = []\n",
    "        for particle in particles:\n",
    "            for part in parts_dict[particle]:\n",
    "                data = hf['data/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()]\n",
    "                shape = data.shape[1:]\n",
    "                axs = tuple( i for i in range(len(shape)) )\n",
    "                mean = np.mean(data, axis=axs, dtype=np.float64)\n",
    "                hf.create_dataset('data/cut_'+str(cut)+'/'+particle+'/'+part+'/norm_param/mean'+postfix, data=mean, dtype=np.float32)\n",
    "                means.append(mean)\n",
    "                nums.append(nums_dict[particle][part])\n",
    "        # get global avg\n",
    "        # calculating factor as this give better precision\n",
    "        total = sum(nums)\n",
    "        gl_mean = 0\n",
    "        for m,n in zip(means,nums):\n",
    "            factor = n/total\n",
    "            gl_mean += m*factor\n",
    "        hf.create_dataset('norm_param_global/cut_'+str(cut)+'/mean'+postfix, data=gl_mean)\n",
    "        # get stds\n",
    "        stds = []\n",
    "        for particle in particles:\n",
    "            for part in parts_dict[particle]:\n",
    "                data = hf['data/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()]\n",
    "                shape = data.shape[1:]\n",
    "                axs = tuple( i for i in range(len(shape)) )\n",
    "                std = np.sqrt( np.mean((data-gl_mean)**2, axis=axs, dtype=np.float64) )\n",
    "                hf.create_dataset('data/cut_'+str(cut)+'/'+particle+'/'+part+'/norm_param/std'+postfix, data=std, dtype=np.float32)\n",
    "                stds.append(std)\n",
    "        # get global std\n",
    "        # a lot of events and big stds, hence intoducing factor should be good\n",
    "        gl_std = 0.\n",
    "        for s,n in zip(stds,nums):\n",
    "            factor = n/total\n",
    "            gl_std += np.power(s,2)*factor\n",
    "        gl_std = np.sqrt(gl_std)\n",
    "        hf.create_dataset('norm_param_global/cut_'+str(cut)+'/std'+postfix, data=gl_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import random as rd\n",
    "import uproot as ur\n",
    "import os\n",
    "h5_in = 'mc_baikal_bundle_pure_string-cut_v4_8OM_mu.h5'\n",
    "h5_out_postfix = '_pure'\n",
    "mean_posfix = ''\n",
    "#mean_posfix = '_mu-'+X+'_nu-'+Y\n",
    "\n",
    "dsets = ['train','test','val']\n",
    "particles = ['mu'] #'mu',\n",
    "cuts_signal = [8]\n",
    "\n",
    "# number of files to use for test and val sets\n",
    "# taking last files\n",
    "num_files_to_test = { 'mu':30} #'mu':30,\n",
    "num_files_to_val = { 'mu':30} #'mu':30,\n",
    "num_files_to_train = { 'mu':1900} #'mu':50,\n",
    " \n",
    "with h5.File('/home/leonov/Baikal/Cut_8_mu/Data/'+h5_in,'r') as hf:\n",
    "    for cut in cuts_signal:\n",
    "        ### define train, test, val sets\n",
    "        # and get numbers of events\n",
    "        # dics structure: dset -> particles\n",
    "        nums_dict = {'train':{}, 'test':{}, 'val':{}}\n",
    "        parts_dict = {'train':{}, 'test':{}, 'val':{}}\n",
    "        total_nums = {'train':{}, 'test':{}, 'val':{}}\n",
    "        total_all = {'train':{}, 'test':{}, 'val':{}}\n",
    "        for particle in particles:\n",
    "            parts = list(hf['data/cut_'+str(cut)+'/'+particle].keys())\n",
    "            # assert that we have enough data\n",
    "            assert len(parts)>(num_files_to_test[particle]+num_files_to_val[particle]+num_files_to_train[particle])\n",
    "            # fill dics\n",
    "            parts_dict['val'][particle] = parts[-num_files_to_val[particle]:]\n",
    "            parts_dict['test'][particle] = parts[-num_files_to_val[particle]-num_files_to_test[particle]:-num_files_to_val[particle]]\n",
    "            parts_dict['train'][particle] = parts[:-num_files_to_val[particle]-num_files_to_test[particle]]\n",
    "            for ds in dsets:\n",
    "                nums_dict[ds][particle] = {}\n",
    "                for part in parts_dict[ds][particle]:\n",
    "                    nums_dict[ds][particle][part] = hf['data/cut_'+str(cut)+'/'+particle+'/'+part+'/data'].shape[0]\n",
    "                total_nums[ds][particle] = sum(nums_dict[ds][particle].values())\n",
    "        for ds in dsets:\n",
    "            total_all[ds] = sum(total_nums[ds].values())\n",
    "    ### normalization\n",
    "    # iterate over cuts\n",
    "        h5_out = 'mc_mu_baikal_norm_cut-'+str(cut)+'_bundle'+h5_out_postfix+'.h5'\n",
    "        with h5.File('/home/leonov/Baikal/Cut_8_mu/Data/'+h5_out,'w') as ho:\n",
    "            # normilize coordiantes\n",
    "            data = hf['geometry/bundle_coordinates'][()]\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            data -= mean\n",
    "            data *= 1./std\n",
    "            ho.create_dataset('geometry/bundle_coordinates', data=data, dtype='float32')\n",
    "            # get mean, std for data\n",
    "            mean = hf['norm_param_global/cut_'+str(cut)+'/mean'+mean_posfix][()]\n",
    "            std = hf['norm_param_global/cut_'+str(cut)+'/std'+mean_posfix][()]\n",
    "            # loop over train/test/val\n",
    "            for ds in dsets:\n",
    "                # normilize          \n",
    "                n = 0\n",
    "                shape = hf['data/cut_'+str(cut)+'/'+particles[0]+'/part_0/data'].shape[1:]\n",
    "                data = np.zeros( np.concatenate(([total_all[ds]],shape), axis=0), dtype='float32' )\n",
    "                shape_chars = hf['ev_chars/cut_'+str(cut)+'/'+particles[0]+'/part_0/data'].shape[1:]\n",
    "                ev_chars = np.zeros( np.concatenate(([total_all[ds]],shape_chars), axis=0), dtype='float32' )\n",
    "                ev_ids = np.full( total_all[ds], 'undefined', dtype='S16' ) \n",
    "                shape = hf['mask/cut_'+str(cut)+'/'+particles[0]+'/part_0/data'].shape[1:]\n",
    "                mask = np.zeros( np.concatenate(([total_all[ds]],shape), axis=0), dtype=bool )                  \n",
    "                for particle in particles:\n",
    "                    for part in parts_dict[ds][particle]:\n",
    "                        num = int(nums_dict[ds][particle][part])\n",
    "                        data[n:n+num] = (hf['data/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()].astype('float32')-mean)/std\n",
    "                        ev_chars[n:n+num] = hf['ev_chars/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()]\n",
    "                        ev_ids[n:n+num] = hf['ev_ids/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()]\n",
    "                        mask[n:n+num] = hf['mask/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()].astype(bool)\n",
    "                        n += num\n",
    "                # shuffle train set\n",
    "                if ds=='train':\n",
    "                    idxs_shuffled = rd.sample(range(total_all[ds]), k=total_all[ds])\n",
    "                else:\n",
    "                    idxs_shuffled = np.arange(0, total_all[ds])        \n",
    "                ho.create_dataset(ds+'/data', data=data[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/ev_chars', data=ev_chars[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/ev_ids', data=ev_ids[idxs_shuffled])\n",
    "                ho.create_dataset(ds+'/mask', data=mask[idxs_shuffled], dtype='float32')\n",
    "                del data\n",
    "                del ev_chars\n",
    "                del ev_ids\n",
    "                # write labels, mask, coordinates\n",
    "                shape = hf['labels/cut_'+str(cut)+'/'+particles[0]+'/part_0/data'].shape[1:]\n",
    "                labels_full = np.zeros( np.concatenate(([total_all[ds]],shape), axis=0), dtype='float16' )\n",
    "                labels_signal_noise = np.zeros( np.concatenate(([total_all[ds]],shape,[2]), axis=0), dtype='float16' )\n",
    "                labels_particle = np.zeros( np.concatenate(([total_all[ds]],[2]), axis=0), dtype='float16' )\n",
    "                labels_track_type = np.zeros( np.concatenate(([total_all[ds]],shape,[3]), axis=0), dtype='float16' )\n",
    "                labels_num_mu = np.zeros( total_all[ds], dtype='float32' )\n",
    "                labels_one_mu = np.zeros( np.concatenate(([total_all[ds]],[2]), axis=0), dtype='float32' )\n",
    "                n = 0\n",
    "                m = np.eye(2)\n",
    "                mm = np.eye(3)\n",
    "                for i,particle in enumerate(particles):\n",
    "                    for part in parts_dict[ds][particle]:\n",
    "                        num = int(nums_dict[ds][particle][part])\n",
    "                        part_labels = hf['labels/cut_'+str(cut)+'/'+particle+'/'+part+'/data'][()]\n",
    "                        labels_full[n:n+num] = part_labels\n",
    "                        # signal-noise labels\n",
    "                        idxs_pos = np.nonzero( part_labels>0 )\n",
    "                        idxs_neg = np.nonzero( part_labels<0 )\n",
    "                        idxs_noise = np.nonzero( part_labels==0 )\n",
    "                        ls_sig = np.zeros( np.concatenate((part_labels.shape,[2]), axis=0), dtype='float16'  )\n",
    "                        ls_sig[idxs_pos] = m[0]\n",
    "                        ls_sig[idxs_neg] = m[0]\n",
    "                        ls_sig[idxs_noise] = m[1]\n",
    "                        labels_signal_noise[n:n+num] = ls_sig                   \n",
    "                        # track lables\n",
    "                        ls_sig = np.zeros( np.concatenate((part_labels.shape,[3]), axis=0), dtype='float16'  )\n",
    "                        ls_sig[idxs_pos] = mm[0]\n",
    "                        ls_sig[idxs_neg] = mm[1]\n",
    "                        ls_sig[idxs_noise] = mm[2]\n",
    "                        labels_track_type[n:n+num] = ls_sig\n",
    "                        # particle labels\n",
    "                        labels_particle[n:n+num] = np.tile( m[i], [num,1] )\n",
    "                        # number of muons in group labels\n",
    "                        nums_mu = np.reshape( part_labels, (part_labels.shape[0],-1) ) % 1000\n",
    "                        labels_num_mu[n:n+num] = np.array([ len(np.unique(l))-1 for l in nums_mu ])\n",
    "                        labels_one_mu[n:n+num] = np.where( np.expand_dims(labels_num_mu[n:n+num],axis=-1)==1,m[0],m[1])                  \n",
    "                        n += num\n",
    "                ho.create_dataset(ds+'/labels_full', data=labels_full[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/labels_signal_noise', data=labels_signal_noise[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/labels_track_type', data=labels_track_type[idxs_shuffled])\n",
    "                ho.create_dataset(ds+'/labels_particle', data=labels_particle[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/labels_num_mu', data=labels_num_mu[idxs_shuffled], dtype='float32')\n",
    "                ho.create_dataset(ds+'/labels_one_mu', data=labels_one_mu[idxs_shuffled], dtype='float32')\n",
    "                # write equal weights\n",
    "                #if ds=='train':\n",
    "                    # l_mask: 0 for auxillary, 1 for for noise, 2 for signal\n",
    "                    #l_mask = np.where( labels_signal_noise[...,0]==1, 2, mask )\n",
    "                    #n_noise = len(np.nonzero(l_mask==1)[0])\n",
    "                    #n_signal = len(np.nonzero(l_mask==2)[0])\n",
    "                    #avg = (n_noise+n_signal)/2\n",
    "                    #w_n = avg/n_noise\n",
    "                    #w_s = avg/n_signal\n",
    "                    #ho.create_dataset('weights/noise-signal', data=[w_n,w_s], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37988b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
